{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spanish to english data\n",
    "\n",
    "import os, unicodedata, re, io\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract = True\n",
    ")\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'\n",
    "\n",
    "# convert unicode files to ascii\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r\"([.,!?¿])\", r\" \\1 \", w)\n",
    "    w = re.sub('\\s{2,}', ' ', w)\n",
    "\n",
    "    # replacing everything with space, except letters, punctuations, ....\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding start and eng tokens\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# create dataset\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "\n",
    "    lines = io.open(path, encoding = 'UTF-8'.read().strip().split('\\n'))\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "# tokenize the sentence and pad the sequence to the same length\n",
    "def tokenize(lang):\n",
    "\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filter = '')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocesing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples = None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c4e3aee882ecdd3db36617fe653367ed255389013010d02bf6e81943e18f4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
